{
 "metadata": {
  "name": "",
  "signature": "sha256:d68fd85a41de179a363440c06481979aee68030fc137ce7c5a93f43f5e36008b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# following http://nilearn.github.io/data_analysis/decoding.html for starters\n",
      "from nilearn import datasets\n",
      "data = datasets.fetch_haxby()\n",
      "print data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'mask_house_little': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/mask8_house_vt.nii.gz'], 'anat': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/anat.nii.gz'], 'mask_house': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/mask8b_house_vt.nii.gz'], 'mask_face': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/mask8b_face_vt.nii.gz'], 'func': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/bold.nii.gz'], 'session_target': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/labels.txt'], 'mask_vt': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/mask4_vt.nii.gz'], 'mask_face_little': ['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/mask8_face_vt.nii.gz']}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "labels = np.recfromcsv(data.session_target[0], delimiter=\" \")\n",
      "target = labels['labels']\n",
      "condition_mask = np.logical_or(labels['labels'] == 'face',\n",
      "                               labels['labels'] == 'cat')\n",
      "target = target[condition_mask]\n",
      "print len(target)\n",
      "print set(target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "216\n",
        "set(['cat', 'face'])\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nilearn.input_data import NiftiMasker\n",
      "nifti_masker = NiftiMasker(mask_img=data.mask_vt[0], standardize=True)\n",
      "fmri_masked = nifti_masker.fit_transform(data.func[0])\n",
      "fmri_masked = fmri_masked[condition_mask]\n",
      "print fmri_masked.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(216, 577)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "svc = SVC(kernel='linear')\n",
      "svc.fit(fmri_masked, target)\n",
      "print \"full train set (overfit!):\", svc.score(fmri_masked, target)\n",
      "\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "cv = KFold(n=len(fmri_masked), n_folds=20)\n",
      "cv_scores = []\n",
      "\n",
      "for train, test in cv:\n",
      "    svc.fit(fmri_masked[train], target[train])\n",
      "    cv_scores.append(svc.score(fmri_masked[test], target[test]))\n",
      "print \"average precision on the test set:\", np.mean(cv_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "full train set (overfit!): 1.0\n",
        "average precision on the test set:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.849545454545\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = datasets.fetch_haxby(n_subjects=6)\n",
      "print data.func\n",
      "data_subjects = []\n",
      "labels_subjects = []\n",
      "for i in xrange(len(data.func)):\n",
      "    fmri_masked = nifti_masker.fit_transform(data.func[i])\n",
      "    labels_subjects.append(np.recfromcsv(data.session_target[i], delimiter=\" \"))\n",
      "    print fmri_masked.shape\n",
      "    data_subjects.append(fmri_masked)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj1/bold.nii.gz', '/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj2/bold.nii.gz', '/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj3/bold.nii.gz', '/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj4/bold.nii.gz', '/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj5/bold.nii.gz', '/Users/gabrielsynnaeve/nilearn_data/haxby2001/subj6/bold.nii.gz']\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1452, 577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import izip\n",
      "masked_subjects = []\n",
      "masked_labels = []\n",
      "for d, l in zip(data_subjects, labels_subjects):\n",
      "    target = l['labels']\n",
      "    condition_mask = l['labels'] != 'rest'\n",
      "    target = target[condition_mask]\n",
      "    fmri_masked = d[condition_mask]\n",
      "    print fmri_masked.shape\n",
      "    print len(target)\n",
      "    print set(target)\n",
      "    masked_subjects.append(fmri_masked)\n",
      "    masked_labels.append(target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(864, 577)\n",
        "864\n",
        "set(['house', 'scrambledpix', 'cat', 'shoe', 'bottle', 'scissors', 'chair', 'face'])\n",
        "(864, 577)\n",
        "864\n",
        "set(['house', 'scrambledpix', 'cat', 'shoe', 'bottle', 'scissors', 'chair', 'face'])\n",
        "(864, 577)\n",
        "864\n",
        "set(['house', 'scrambledpix', 'cat', 'shoe', 'bottle', 'scissors', 'chair', 'face'])\n",
        "(864, 577)\n",
        "864\n",
        "set(['house', 'scrambledpix', 'face', 'shoe', 'bottle', 'scissors', 'chair', 'cat'])\n",
        "(792, 577)\n",
        "792\n",
        "set(['house', 'scrambledpix', 'cat', 'shoe', 'bottle', 'scissors', 'chair', 'face'])\n",
        "(864, 577)\n",
        "864\n",
        "set(['house', 'scrambledpix', 'face', 'shoe', 'bottle', 'scissors', 'chair', 'cat'])\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N_DEV_SUBJECTS = 2\n",
      "N_TEST_SUBJECTS = 2\n",
      "FULLTRAINSET = False\n",
      "\n",
      "train_data = np.concatenate(masked_subjects[:-(N_TEST_SUBJECTS+N_DEV_SUBJECTS)], axis=0)\n",
      "train_imgs = np.concatenate(masked_labels[:-(N_TEST_SUBJECTS+N_DEV_SUBJECTS)], axis=0)\n",
      "train_subjs = np.concatenate([np.ones(masked_labels[j].shape[0])*j for j in xrange(len(masked_labels[:-(N_TEST_SUBJECTS+N_DEV_SUBJECTS)]))], axis=0)\n",
      "if FULLTRAINSET:\n",
      "    train_data = np.concatenate(masked_subjects, axis=0)\n",
      "    train_imgs = np.concatenate(masked_labels, axis=0)\n",
      "    train_subjs = np.concatenate([np.ones(masked_labels[j].shape[0])*j for j in xrange(len(masked_labels))], axis=0)\n",
      "print train_data.shape\n",
      "print train_imgs.shape\n",
      "print train_subjs.shape\n",
      "#print data.mean(axis=0), data.std(axis=0)\n",
      "dev_data = np.concatenate(masked_subjects[-(N_TEST_SUBJECTS+N_DEV_SUBJECTS):-N_TEST_SUBJECTS], axis=0)\n",
      "dev_imgs = np.concatenate(masked_labels[-(N_TEST_SUBJECTS+N_DEV_SUBJECTS):-N_TEST_SUBJECTS], axis=0)\n",
      "dev_subjs = np.concatenate([np.ones(masked_labels[j].shape[0])*j for j in xrange(len(masked_labels)-(N_TEST_SUBJECTS+N_DEV_SUBJECTS), len(masked_labels)-N_TEST_SUBJECTS)], axis=0)\n",
      "print dev_data.shape\n",
      "print dev_imgs.shape\n",
      "print dev_subjs.shape\n",
      "test_data = np.concatenate(masked_subjects[-N_TEST_SUBJECTS:], axis=0)\n",
      "test_imgs = np.concatenate(masked_labels[-N_TEST_SUBJECTS:], axis=0)\n",
      "test_subjs = np.concatenate([np.ones(masked_labels[j].shape[0])*j for j in xrange(len(masked_labels)-N_TEST_SUBJECTS, len(masked_labels))], axis=0)\n",
      "print test_data.shape\n",
      "print test_imgs.shape\n",
      "print test_subjs.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1728, 577)\n",
        "(1728,)\n",
        "(1728,)\n",
        "(1728, 577)\n",
        "(1728,)\n",
        "(1728,)\n",
        "(1656, 577)\n",
        "(1656,)\n",
        "(1656,)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dataset_iterators import DatasetAB2OSamplingIteratorFromLabels\n",
      "# set BATCH SIZE at 1000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GT 650M\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_iterator = DatasetAB2OSamplingIteratorFromLabels(train_data, train_imgs, train_subjs, batch_size=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "finished initializing the iterator\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_set_iterator = DatasetAB2OSamplingIteratorFromLabels(dev_data, dev_imgs, dev_subjs, batch_size=1000)\n",
      "test_set_iterator = DatasetAB2OSamplingIteratorFromLabels(test_data, test_imgs, test_subjs, batch_size=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "finished initializing the iterator\n",
        "finished initializing the iterator\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from layers import Linear, ReLU, SigmoidLayer, SoftPlus\n",
      "from nnet_archs import ABNeuralNet2Outputs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_ins = train_data.shape[1]\n",
      "n_outs = 100  # Dimension of the embedding\n",
      "numpy_rng = np.random.RandomState(123)\n",
      "nnet = ABNeuralNet2Outputs(numpy_rng=numpy_rng,\n",
      "                n_ins=n_ins,\n",
      "                layers_types=[ReLU, ReLU, ReLU],\n",
      "                #layers_sizes=[500, 500],\n",
      "                layers_sizes=[100, 100],\n",
      "                n_outs=n_outs,\n",
      "                loss='cos_cos2',\n",
      "                #loss='dot_prod',\n",
      "                rho=0.90,\n",
      "                eps=1.E-6,\n",
      "                max_norm=0.,\n",
      "                debugprint=0)\n",
      "                #debugprint=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_fn = nnet.get_adadelta_trainer()\n",
      "train_scoref_p = nnet.score_classif_same_diff_word_separated(train_set_iterator)\n",
      "valid_scoref_p = nnet.score_classif_same_diff_word_separated(valid_set_iterator)\n",
      "test_scoref_p = nnet.score_classif_same_diff_word_separated(test_set_iterator)\n",
      "train_scoref_s = nnet.score_classif_same_diff_spkr_separated(train_set_iterator)\n",
      "valid_scoref_s = nnet.score_classif_same_diff_spkr_separated(valid_set_iterator)\n",
      "test_scoref_s = nnet.score_classif_same_diff_spkr_separated(test_set_iterator)\n",
      "data_iterator = train_set_iterator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time, cPickle\n",
      "best_validation_loss = np.inf\n",
      "test_score = 0.\n",
      "start_time = time.clock()\n",
      "output_file_name = \"huxby_2embs_nnet\"\n",
      "epoch = 0\n",
      "max_epochs = 100\n",
      "timer = None\n",
      "\n",
      "while (epoch < max_epochs):\n",
      "    epoch = epoch + 1\n",
      "    avg_costs = []\n",
      "    avg_params_gradients_updates = []\n",
      "    timer = time.time()\n",
      "    for iteration, (x, y) in enumerate(data_iterator):\n",
      "        avg_cost = train_fn(x[0], x[1], y[0], y[1])\n",
      "        if type(avg_cost) == list:\n",
      "            avg_costs.append(avg_cost[0])\n",
      "        else:\n",
      "            avg_costs.append(avg_cost)\n",
      "    print('  epoch %i took %f seconds' % (epoch, time.time() - timer))\n",
      "    avg_cost = np.mean(avg_costs)\n",
      "    if np.isnan(avg_cost):\n",
      "        print(\"avg costs is NaN so we're stopping here!\")\n",
      "        break\n",
      "    print('  epoch %i, avg costs %f' % \\\n",
      "          (epoch, avg_cost))\n",
      "    tmp_train = zip(*train_scoref_p())\n",
      "    print('  epoch %i, training sim same pics %f, diff pics %f' % \\\n",
      "          (epoch, np.mean(tmp_train[0]), np.mean(tmp_train[1])))\n",
      "    tmp_train = zip(*train_scoref_s())\n",
      "    print('  epoch %i, training sim same subjs %f, diff subjs %f' % \\\n",
      "          (epoch, np.mean(tmp_train[0]), np.mean(tmp_train[1])))\n",
      "    # we check the validation loss on every epoch\n",
      "    validation_losses_p = zip(*valid_scoref_p())\n",
      "    validation_losses_s = zip(*valid_scoref_s())\n",
      "    this_validation_loss = 0.25*(1.-np.mean(validation_losses_p[0])) +\\\n",
      "            0.25*np.mean(validation_losses_p[1]) +\\\n",
      "            0.25*(1.-np.mean(validation_losses_s[0])) +\\\n",
      "            0.25*np.mean(validation_losses_s[1])\n",
      "\n",
      "    print('  epoch %i, valid sim same pics %f, diff pics %f' % \\\n",
      "          (epoch, np.mean(validation_losses_p[0]), np.mean(validation_losses_p[1])))\n",
      "    print('  epoch %i, valid sim same subjs %f, diff subjs %f' % \\\n",
      "          (epoch, np.mean(validation_losses_s[0]), np.mean(validation_losses_s[1])))\n",
      "    # if we got the best validation score until now\n",
      "    if this_validation_loss < best_validation_loss:\n",
      "        with open(output_file_name + '.pickle', 'wb') as f:\n",
      "            cPickle.dump(nnet, f, protocol=-1)\n",
      "        # save best validation score and iteration number\n",
      "        best_validation_loss = this_validation_loss\n",
      "        # test it on the test set\n",
      "        test_losses_p = zip(*test_scoref_p())\n",
      "        test_losses_s = zip(*test_scoref_s())\n",
      "        print('  epoch %i, test sim same pics %f, diff pics %f' % \\\n",
      "              (epoch, np.mean(test_losses_p[0]), np.mean(test_losses_p[1])))\n",
      "        print('  epoch %i, test sim same subjs %f, diff subjs %f' % \\\n",
      "              (epoch, np.mean(test_losses_s[0]), np.mean(test_losses_s[1])))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  epoch 1 took 2.035495 seconds\n",
        "  epoch 1, avg costs 0.439071\n",
        "  epoch 1, training sim same pics 0.382567, diff pics 0.375442"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}